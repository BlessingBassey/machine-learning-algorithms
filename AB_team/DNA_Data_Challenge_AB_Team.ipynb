{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.16 (default, Apr  6 2019, 01:42:57) \n",
      "[GCC 8.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.4'"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model as lm\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm.py\n",
    "import numpy as np  # for handling multi-dimensional array operation\n",
    "import pandas as pd  # for reading data from csv \n",
    "#import statsmodels.api as sm  # for finding the p-value\n",
    "from sklearn.preprocessing import MinMaxScaler  # for normalization\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train= pd.read_csv('Xtr_mat100.csv',header=None)\n",
    "X_test = pd.read_csv('Xte_mat100.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1, ...,  1,  1,  1])"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = pd.read_csv('Ytr.csv')\n",
    "y = Y_train['Bound'].to_numpy()\n",
    "y=2*y-1\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float(data):\n",
    "    X=[]\n",
    "    n = data.shape[0]\n",
    "    k=10\n",
    "    for i in data.index:\n",
    "        lis = data.iloc[i, :].str.split()\n",
    "        newlis = np.array(lis.tolist(),dtype=float)\n",
    "        newlis = newlis.reshape(1,-1)\n",
    "        X.append(newlis)\n",
    "   \n",
    "    return X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr=string_to_float(X_train)\n",
    "Xte=string_to_float(X_test)\n",
    "New_Xtr = np.array(Xtr).squeeze()\n",
    "New_Xte = np.array(Xte).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression (RR)\n",
    "def solveRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "\n",
    "    A = X.T.dot(X)\n",
    "    # Adjust diagonal due to Ridge\n",
    "    A[np.diag_indices_from(A)] += lam * n\n",
    "    b = X.T.dot(y)\n",
    "\n",
    "    # Hint:\n",
    "    beta = np.linalg.solve(A, b)\n",
    "    # Finds solution to the linear system Ax = b\n",
    "    return (beta) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ridge Regression (WRR)\n",
    "def solveWRR(y, X, w, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == len(w) == n)\n",
    "\n",
    "    y1 = np.sqrt(w) * y\n",
    "    X1 = (np.sqrt(w) * X.T).T\n",
    "    # Hint:\n",
    "    # Find y1 and X1 such that:\n",
    "    beta = solveRR(y1, X1, lam)\n",
    "    return (beta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Ridge Regression (LRR)\n",
    "def solveLRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    #Parameters\n",
    "    max_iter=100\n",
    "    eps=1e-3\n",
    "    sigmoid = lambda a: 1/(1 + np.exp(-a))\n",
    "    \n",
    "    #Initialize\n",
    "    beta = np.zeros(p)\n",
    "    \n",
    "            \n",
    "    # Hint: Use IRLS\n",
    "    for i in range(max_iter):\n",
    "        beta_old = beta\n",
    "        f= X.dot(beta_old)\n",
    "        W=sigmoid(f)*sigmoid(-f)\n",
    "        z=f.T+y/sigmoid(y*f)\n",
    "        beta = solveWRR(z, X, W, 2*lam)\n",
    "    # Break condition\n",
    "        if np.sum((beta-beta_old)**2) < eps:\n",
    "            break\n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(Data_tr, y,test_size=0.7,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model and compute its parameters\n",
    "lam = 0.1\n",
    "beta = solveLRR(Y_train, X_train, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities and classes\n",
    "probas_pred = 1/(1+np.exp(-beta.T.dot(X_test.T)))\n",
    "y_pred = np.round(probas_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model's performance:\n",
      "Accuracy: 49.57%\n",
      "AUC: 49.17%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Our model's performance:\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(Y_test, y_pred)))\n",
    "print('AUC: {:.2%}'.format(roc_auc_score(Y_test, probas_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(New_Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = MinMaxScaler().fit_transform(New_Xtr)\n",
    "data = pd.DataFrame(data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1         2         3         4      5         6         7   \\\n",
       "0  0.090909  0.125  0.142857  0.428571  0.166667  0.375  0.166667  0.000000   \n",
       "1  0.000000  0.125  0.285714  0.000000  0.333333  0.250  0.000000  0.166667   \n",
       "2  0.181818  0.000  0.000000  0.000000  0.666667  0.125  0.166667  0.166667   \n",
       "3  0.000000  0.250  0.428571  0.000000  0.333333  0.000  0.500000  0.166667   \n",
       "4  0.090909  0.500  0.000000  0.428571  0.166667  0.125  0.000000  0.166667   \n",
       "\n",
       "         8         9   ...        90     91   92        93   94        95  \\\n",
       "0  0.000000  0.166667  ...  0.181818  0.000  0.0  0.000000  0.2  0.000000   \n",
       "1  0.333333  0.166667  ...  0.272727  0.000  0.0  0.000000  0.0  0.000000   \n",
       "2  0.000000  0.333333  ...  0.000000  0.125  0.2  0.000000  0.0  0.090909   \n",
       "3  0.333333  0.000000  ...  0.090909  0.125  0.0  0.000000  0.0  0.000000   \n",
       "4  0.166667  0.000000  ...  0.000000  0.000  0.2  0.272727  0.4  0.272727   \n",
       "\n",
       "         96        97        98        99  \n",
       "0  0.000000  0.117647  0.000000  0.000000  \n",
       "1  0.000000  0.294118  0.000000  0.166667  \n",
       "2  0.000000  0.117647  0.000000  0.166667  \n",
       "3  0.000000  0.058824  0.142857  0.333333  \n",
       "4  0.142857  0.058824  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
    "    \n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])\n",
    "    d = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = W if max(0, d) == 0 else (W - (reg_strength * Y_batch * X_batch))\n",
    "    \n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs): \n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd1(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        print('After shffle', X)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "   \n",
    "    print(\"splitting dataset into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = tts(dtrain, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "\n",
    "    # train the model\n",
    "    print(\"training started...\")\n",
    "    W = sgd1(X_train, y_train)\n",
    "    print(\"training finished.\")\n",
    "    print(\"weights are: {}\".format(W))\n",
    "    \n",
    "    \n",
    "    # testing the model on test set\n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(W, X_test[i])) #model\n",
    "        y_test_predicted = np.append(y_test_predicted, yp)\n",
    "    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
    "    print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting dataset into train and test sets...\n",
      "training started...\n",
      "('After shffle', array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n",
      "Epoch is:1 and Cost is: 5180.0\n",
      "('After shffle', array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]))\n",
      "Epoch is:2 and Cost is: 5180.0\n",
      "training finished.\n",
      "weights are: [0. 0. 0. ... 0. 0. 0.]\n",
      "accuracy on test dataset: 0.514\n",
      "recall on test dataset: 0.994186046512\n",
      "precision on test dataset: 0.994186046512\n"
     ]
    }
   ],
   "source": [
    "#set hyper-parameters and call init\n",
    "#hyper-parameters are normally tuned using cross-validation\n",
    "#but following work good enough\n",
    "reg_strength = 10000 # regularization strength\n",
    "learning_rate = 0.001\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing with spectrum kernel and one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('Xtr.csv')\n",
    "df_te = pd.read_csv('Xte.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "kmer_size = 3\n",
    "\n",
    "def base2int(c):\n",
    "    return {'A': 0, 'C':1, 'G':2, 'T':3}.get(c,0)\n",
    "\n",
    "def get_kmers(sequence, kmer_size=3):\n",
    "    return [sequence[i: i + kmer_size] for i in range(len(sequence) - kmer_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base2int(c):\n",
    "    return {'A': 0, 'C': 1, 'G': 2, 'T': 3}.get(c,0)\n",
    "\n",
    "def index(kmer):\n",
    "    #Transform the kmer into sequence of character indices\n",
    "    base_indices = np.array([base2int(base) for base in kmer])\n",
    "    multiplier = 4**np.arange(len(kmer))  #[4**0, 4**1, 4**2, ...]\n",
    "    kmer_index = multiplier.dot(base_indices)\n",
    "    \n",
    "    return kmer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrum_embedding1(sequence):\n",
    "    kmers = get_kmers(sequence)\n",
    "    kmer_indices = [index(kmer) for kmer in kmers]\n",
    "    one_hot_vector = np.zeros(4**kmer_size)\n",
    "    for kmer_index in kmer_indices:\n",
    "        one_hot_vector[kmer_index] +=1\n",
    "    return one_hot_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def spectrum_embedding(data,k, train = True):\n",
    "    out = []\n",
    "    for i in range(len(data)):\n",
    "        line = data.iloc[i]['seq']\n",
    "        kmers = get_kmers(line,k)\n",
    "        out.append(kmers)\n",
    "        \n",
    "    data = pd.DataFrame(data=out,columns=['txt'+str(i) for i in range(len(kmers))])\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    encoded_data = encoder.fit_transform(data)\n",
    "    return data,encoded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def spectrum_embedding(data, data_test, k):\n",
    "    out, out2 = [], []\n",
    "    for i in range(len(data)):\n",
    "        line = data.iloc[i]['seq']\n",
    "        kmers = get_kmers(line,k)\n",
    "        out.append(kmers)\n",
    "        \n",
    "    for i in range(len(data_test)):\n",
    "        line = data_test.iloc[i]['seq']\n",
    "        kmers = get_kmers(line,k)\n",
    "        out2.append(kmers)\n",
    "        \n",
    "    data = pd.DataFrame(data=out,columns=['txt'+str(i) for i in range(len(kmers))])\n",
    "    data_test = pd.DataFrame(data=out2,columns=['txt'+str(i) for i in range(len(kmers))])\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    encoded_data = encoder.fit(data)\n",
    "        \n",
    "    data_train = encoded_data.transform(data)\n",
    "    data_test = encoded_data.transform(data_test)\n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain, dtest = spectrum_embedding(df_tr, df_te, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_tr = [spectrum_embedding1(i) for i in df_tr['seq']]\n",
    "Data_te = [spectrum_embedding1(j) for j in df_te['seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_tr = pd.DataFrame(Data_tr) \n",
    "Data_te = pd.DataFrame(Data_te) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_tr = Data_tr.reset_index()\n",
    "Data_te = Data_te.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_tr = Data_tr.rename(columns={'index':'Id'})\n",
    "Data_te = Data_te.rename(columns={'index':'Id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_tr = Data_tr.to_numpy()\n",
    "Data_te = Data_te.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(Data_tr, y, test_size = 0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "\n",
    "def cvxopt_qp(P, q, G, h, A, b):\n",
    "    P = .5 * (P + P.T)\n",
    "    cvx_matrices = [\n",
    "        cvxopt.matrix(M) if M is not None else None for M in [P, q, G, h, A, b] \n",
    "    ]\n",
    "    #cvxopt.solvers.options['show_progress'] = False\n",
    "    solution = cvxopt.solvers.qp(*cvx_matrices, options={'show_progress': False})\n",
    "    return np.array(solution['x']).flatten()\n",
    "\n",
    "solve_qp = cvxopt_qp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_element_wise(x, y, sigma=1):\n",
    "    '''\n",
    "    returns the RBF (Gaussian) kernel k(x, y)\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    x and y are p-dimensional vectors \n",
    "    '''\n",
    "    K = np.exp(- np.sum((x - y)**2) / (2 * sigma ** 2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, sigma=10):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    '''\n",
    "    # For loop with rbf_kernel_element works but is slow in python\n",
    "    # Use matrix operations!\n",
    "    X2_norm = np.sum(X2 ** 2, axis=-1)\n",
    "    X1_norm = np.sum(X1 ** 2, axis=-1)\n",
    "    gamma = 1 / (2 * sigma ** 2)\n",
    "    K = np.exp(- gamma * (X1_norm + X2_norm - 2 * np.dot(X1, X2.T)))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_from_median(X):\n",
    "    '''\n",
    "    Returns the median of ||Xi-Xj||\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    X: (n, p) matrix\n",
    "    '''\n",
    "    pairwise_diff = X[:, :, None] - X[:, :, None].T\n",
    "    pairwise_diff *= pairwise_diff\n",
    "    euclidean_dist = np.sqrt(pairwise_diff.sum(axis=1))\n",
    "    return np.median(euclidean_dist)\n",
    "\n",
    "#print(sigma_from_median(New_Xtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidge():\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    \n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    '''\n",
    "    def __init__(self, sigma=None, lambd=0.1):\n",
    "        self.kernel = rbf_kernel\n",
    "        self.sigma = sigma\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        \n",
    "        # Compute default sigma from data\n",
    "        if self.sigma is None:\n",
    "            self.sigma = sigma_from_median(X)\n",
    "        \n",
    "        A = self.kernel(X, X, sigma=self.sigma) + n * self.lambd * np.eye(n)\n",
    "        \n",
    "        ## self.alpha = (K + n lambda I)^-1 y\n",
    "        # Solution to A x = y\n",
    "        self.alpha = np.linalg.solve(A , y)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Prediction rule: \n",
    "        K_x = self.kernel(X, self.X_train, sigma=self.sigma)\n",
    "        return K_x.dot(self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(New_Xtr, y, test_size = 0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = KernelRidge(lambd=0.1, sigma=None)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, sigma=10):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    where K is the RBF kernel with parameter sigma\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    sigma: float\n",
    "    '''\n",
    "    # For loop with rbf_kernel_element works but is slow in python\n",
    "    # Use matrix operations!\n",
    "    X2_norm = np.sum(X2 ** 2, axis = -1)\n",
    "    X1_norm = np.sum(X1 ** 2, axis = -1)\n",
    "    gamma = 1 / (2 * sigma ** 2)\n",
    "    K = np.exp(- gamma * (X1_norm[:, None] + X2_norm[None, :] - 2 * np.dot(X1, X2.T)))\n",
    "    return K\n",
    "\n",
    "def sigma_from_median(X):\n",
    "    '''\n",
    "    Returns the median of ||Xi-Xj||\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    X: (n, p) matrix\n",
    "    '''\n",
    "    pairwise_diff = X[:, :, None] - X[:, :, None].T\n",
    "    pairwise_diff *= pairwise_diff\n",
    "    euclidean_dist = np.sqrt(pairwise_diff.sum(axis=1))\n",
    "    return np.median(euclidean_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelMethodBase(object):\n",
    "    '''\n",
    "    Base class for kernel methods models\n",
    "    \n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    '''\n",
    "    kernels_ = {\n",
    "        'linear': linear_kernel,\n",
    "        'polynomial': polynomial_kernel,\n",
    "        'rbf': rbf_kernel,\n",
    "        # 'custom_kernel': custom_kernel, # Your kernel\n",
    "    }\n",
    "    def __init__(self, kernel='linear', **kwargs):\n",
    "        self.kernel_name = kernel\n",
    "        self.kernel_function_ = self.kernels_[kernel]\n",
    "        self.kernel_parameters = self.get_kernel_parameters(**kwargs)\n",
    "        \n",
    "    def get_kernel_parameters(self, **kwargs):\n",
    "        params = {}\n",
    "        if self.kernel_name == 'rbf':\n",
    "            params['sigma'] = kwargs.get('sigma', 1.)\n",
    "        if self.kernel_name == 'polynomial':\n",
    "            params['degree'] = kwargs.get('degree', 2)\n",
    "        # if self.kernel_name == 'custom_kernel':\n",
    "        #     params['parameter_1'] = kwargs.get('parameter_1', None)\n",
    "        #     params['parameter_2'] = kwargs.get('parameter_2', None)\n",
    "        return params\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X1, X2):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    where K is the linear kernel\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    '''\n",
    "    return X1.dot(X2.T)\n",
    "\n",
    "def polynomial_kernel(X1, X2, degree=2):\n",
    "    '''\n",
    "    Returns the kernel matrix K(X1_i, X2_j): size (n1, n2)\n",
    "    where K is the polynomial kernel of degree `degree`\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    X1: an (n1, p) matrix\n",
    "    X2: an (n2, p) matrix\n",
    "    degree: int\n",
    "    '''\n",
    "    return (1 + linear_kernel(X1, X2))**degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Ridge Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelRidgeRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        n = len(self.y_train)\n",
    "        \n",
    "        A = self.kernel_function_(X, X, **self.kernel_parameters)\n",
    "        A[np.diag_indices_from(A)] += self.lambd * n\n",
    "        # self.alpha = (K + n lambda I)^-1 y\n",
    "        self.alpha = np.linalg.solve(A , self.y_train)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        K_x = self.kernel_function_(X, self.X_train, **self.kernel_parameters)\n",
    "        return np.dot(K_x,self.alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.decision_function(X)\n",
    "        predicted_classes = np.where(proba <=0.5, 0, 1)\n",
    "        #pred= np.where(predicted_classes==-1,0,1)\n",
    "        return predicted_classes\n",
    "    \n",
    "    def error(self,ypred, ytrue):\n",
    "        e = (ypred == ytrue).mean()\n",
    "        return e\n",
    "    \n",
    "    \n",
    "class WeightedKernelRidgeRegression(KernelRidgeRegression):\n",
    "    '''\n",
    "    Weighted Kernel Ridge Regression\n",
    "    \n",
    "    This is just used for the KernelLogistic following up\n",
    "    '''\n",
    "    def fit(self, K, y, sample_weights=None):\n",
    "\n",
    "        self.y_train = y\n",
    "        n = len(self.y_train)\n",
    "        \n",
    "        w = np.ones_like(self.y_train) if sample_weights is None else sample_weights\n",
    "        W = np.diag(np.sqrt(w))\n",
    "        \n",
    "        A = W.dot(K).dot(W)\n",
    "        A[np.diag_indices_from(A)] += self.lambd * n\n",
    "        # self.alpha = W (K + n lambda I)^-1 W y\n",
    "        self.alpha = W.dot(np.linalg.solve(A , W.dot(self.y_train)))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeClassifier(KernelRidgeRegression):\n",
    "    '''\n",
    "    Kernel Ridge Classification\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(dtrain, y, test_size = 0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 50.40%\n"
     ]
    }
   ],
   "source": [
    "kernel = 'rbf'\n",
    "lambd = 0.1\n",
    "sigma = 2\n",
    "model = KernelRidgeRegression(\n",
    "        kernel=kernel,\n",
    "        lambd=lambd,\n",
    "        sigma=sigma\n",
    "    ).fit(X_train, y_train)\n",
    "ypred = model.predict(X_test)\n",
    "print('Test error: {:.2%}'.format(error(ypred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(dtrain, y, test_size = 0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class KernelLogisticRegression(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, lambd=0.1, **kwargs):\n",
    "        self.lambd = lambd\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelLogisticRegression, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, max_iter=100, tol=1e-5):\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        K = self.kernel_function_(X, X, **self.kernel_parameters)\n",
    "        \n",
    "        # IRLS\n",
    "        WKRR = WeightedKernelRidgeRegression(\n",
    "            lambd=self.lambd,\n",
    "            kernel=self.kernel_name,\n",
    "            **self.kernel_parameters\n",
    "        )\n",
    "        # Initialize\n",
    "        alpha = np.zeros_like(self.y_train)\n",
    "        # Iterate until convergence or max iterations\n",
    "        for n_iter in range(max_iter):\n",
    "            alpha_old = alpha\n",
    "            f = K.dot(alpha_old)\n",
    "            w = sigmoid(f) * sigmoid(-f)\n",
    "            z = f + y / sigmoid(-y*f)\n",
    "            alpha = WKRR.fit(K, z, sample_weights=w).alpha\n",
    "            # Break condition (achieved convergence)\n",
    "            if np.sum((alpha-alpha_old)**2) < tol:\n",
    "                break\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "\n",
    "        return self\n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        K_x = self.kernel_function_(X_test, self.X_train, **self.kernel_parameters)\n",
    "        # probability of y=1(between o and 1)\n",
    "        return sigmoid(K_x.dot(self.alpha))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas =self.decision_function(X)\n",
    "        predicted_classes = np.where(probas < 0.5, -1,1)\n",
    "        return predicted_classes\n",
    "    \n",
    "    def error(self,ypred, ytrue):\n",
    "        e = (ypred == ytrue).mean()\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(dtrain, y, test_size = 0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 49.60%\n"
     ]
    }
   ],
   "source": [
    "kernel = 'linear'\n",
    "degree = 2\n",
    "sigma = 10.\n",
    "lambd = 0.001\n",
    "fig_title = 'Logistic Regression, {} Kernel'.format(kernel)\n",
    "\n",
    "model = KernelLogisticRegression(lambd=lambd, kernel=kernel, sigma=sigma, degree=degree)\n",
    "y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "#plot_decision_function(model, X_train, y_train, title=fig_title)\n",
    "print('Test error: {:.2%}'.format(error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(Data_tr, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_dual_soft_to_qp_kernel(K, y, C=1):\n",
    "    n = K.shape[0]\n",
    "    assert (len(y) == n)\n",
    "        \n",
    "    # Dual formulation, soft margin\n",
    "    P = np.diag(y).dot(K).dot(np.diag(y))\n",
    "\n",
    "    # As a regularization, we add epsilon * identity to P\n",
    "    eps = 1e-12\n",
    "    P =P+ eps * np.eye(n)\n",
    "    q = - np.ones(n)\n",
    "    G = np.vstack([-np.eye(n), np.eye(n)])\n",
    "    h = np.hstack([np.zeros(n), C * np.ones(n)])\n",
    "    A = y[np.newaxis,:]\n",
    "    A = A.astype('float')\n",
    "    b = np.array([0.])\n",
    "    return P, q, G, h, A, b\n",
    "\n",
    "#K = polynomial_kernel(Xtrain, Xtrain)\n",
    "#alphas = solve_qp(*svm_dual_soft_to_qp_kernel(K, y, C=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVM(KernelMethodBase):\n",
    "    '''\n",
    "    Kernel SVM Classification\n",
    "    \n",
    "    Methods\n",
    "    ----\n",
    "    fit\n",
    "    predict\n",
    "    '''\n",
    "    def __init__(self, C=0.1, **kwargs):\n",
    "        self.C = C\n",
    "        # Python 3: replace the following line by\n",
    "        # super().__init__(**kwargs)\n",
    "        super(KernelSVM, self).__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, tol=1e-3):\n",
    "        n, p = X.shape\n",
    "        assert (n == len(y))\n",
    "    \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        # Kernel matrix\n",
    "        \n",
    "        K = self.kernel_function_(X,X, **self.kernel_parameters)\n",
    "        \n",
    "        # Solve dual problem\n",
    "        self.alpha = solve_qp(*svm_dual_soft_to_qp_kernel(K, y, C=self.C))\n",
    "        \n",
    "        # Compute support vectors and bias b\n",
    "        #sv = (self.alpha > 1e-3)\n",
    "        sv = np.logical_and(self.alpha > tol, (self.C - self.alpha > tol))\n",
    "        \n",
    "        self.beta = self.alpha*y\n",
    "        \n",
    "        self.X_sv = X[sv]\n",
    "        self.Y_sv = y[sv]\n",
    "\n",
    "        self.bias = y[sv]-K[sv].dot(self.alpha*y)#self.X_sv.dot(X.T).dot(self.beta)\n",
    "        \n",
    "        self.bias = self.bias.mean()\n",
    "        self.support_vector_indices = np.nonzero(sv)[0]\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        return self.kernel_function_(X, self.X_train,**self.kernel_parameters).dot(self.beta)+self.bias\n",
    "    \n",
    "    def ensemble(self,X):\n",
    "        prob = self.decision_function(X)\n",
    "           \n",
    "        return prob\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        prob = np.sign(self.decision_function(X))\n",
    "        \n",
    "        predicted_classes = np.where(prob==-1, 0, 1)\n",
    "\n",
    "        \n",
    "        return predicted_classes\n",
    "    \n",
    "    def error(self,ypred, ytrue):\n",
    "        e = (ypred == ytrue).mean()\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 82.00%\n"
     ]
    }
   ],
   "source": [
    "kernel = 'rbf'\n",
    "degree = 2\n",
    "sigma = 5.\n",
    "C = 10.\n",
    "model = KernelSVM(C=C, kernel=kernel, sigma=sigma, degree=degree)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred =model.predict(X_test)\n",
    "\n",
    "print('Test error: {:.2%}'.format(error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation  @kernelridgeregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurracy fold {i}: set([0.6125])\n",
      "accurracy fold {i}: set([0.6775])\n",
      "accurracy fold {i}: set([0.665])\n",
      "accurracy fold {i}: set([0.6575])\n",
      "accurracy fold {i}: set([0.67])\n",
      "Average accuracy is: set([0.67])\n"
     ]
    }
   ],
   "source": [
    "kernel = 'rbf'\n",
    "lam = 0.001\n",
    "sigma = 10.\n",
    "C=10.\n",
    "degree=2\n",
    "y=(y+1)/2\n",
    "\n",
    "X_cross=dtrain\n",
    "y_cross=y\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=5)\n",
    "accuracy = []\n",
    "for i, (train_index, validate_index) in enumerate(kfold.split(X_cross)):\n",
    "    X_train, y_train = X_cross[train_index], y_cross[train_index]\n",
    "    X_valid, y_valid = X_cross[validate_index], y_cross[validate_index]\n",
    "    \n",
    "    model_curr = KernelRidgeRegression(\n",
    "        kernel=kernel,\n",
    "        lambd=lam,\n",
    "        sigma=sigma\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    y_hat = model_curr.predict(X_valid)\n",
    "    accuracy = model_curr.error(y_hat,y_valid)\n",
    "    \n",
    "    print 'accurracy fold {i}:', {accuracy}\n",
    "print 'Average accuracy is:', {np.mean(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = pd.read_csv('Xtr.csv')\n",
    "d_test = pd.read_csv('Xte.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqq_tra = d_train['seq']\n",
    "seqq_tes = d_test['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tra = map(lambda x: [x[i:i+1] for i in range(0, len(x), 1)],seqq_tra )\n",
    "result_tes = map(lambda x: [x[i:i+1] for i in range(0, len(x), 1)],seqq_tes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra = list(result_tra)\n",
    "df_tes = list(result_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_tra = pd.DataFrame(df_tra)\n",
    "dff_tes = pd.DataFrame(df_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "hot_tra = pd.DataFrame(enc.fit_transform(dff_tra).toarray())\n",
    "hot_tes = pd.DataFrame(enc.fit_transform(dff_tes).toarray())\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer2 = CountVectorizer(analyzer='word',min_df=1, ngram_range=(2, 2))\n",
    "# X2 = vectorizer2.fit_transform(hot_tra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 404 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    ...  394  395  396  \\\n",
       "995  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "996  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  ...  0.0  0.0  1.0   \n",
       "997  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0   \n",
       "998  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  ...  0.0  1.0  0.0   \n",
       "999  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0   \n",
       "\n",
       "     397  398  399  400  401  402  403  \n",
       "995  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "996  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "997  0.0  0.0  1.0  0.0  0.0  0.0  1.0  \n",
       "998  0.0  0.0  1.0  0.0  1.0  0.0  0.0  \n",
       "999  0.0  0.0  1.0  0.0  1.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 404 columns]"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hot_tes = pd.get_dummies(dff_tes)\n",
    "hot_tes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    \n",
    "    def fit(self, X, y, ls=0.01):\n",
    "        self.ls = ls\n",
    "        self.y_classes, y_counts = np.unique(y, return_counts=True)\n",
    "        self.x_classes = [np.unique(x) for x in X.T]\n",
    "        self.phi_y = 1.0 * y_counts/y_counts.sum()\n",
    "        self.phi_x = self.mean_X(X, y)\n",
    "        self.c_x = self.count_x(X, y)\n",
    "        return self\n",
    "    \n",
    "    def mean_X(self, X, y):\n",
    "        return [[self.ls_mean_x(X, y, k, j) for j in range(len(self.x_classes))] for k in self.y_classes]\n",
    "    \n",
    "    def ls_mean_x(self, X, y, k, j):\n",
    "        x_data = (X[:,j][y==k].reshape(-1,1) == self.x_classes[j])\n",
    "        return (x_data.sum(axis=0) + self.ls ) / (len(x_data) + (len(self.x_classes) * self.ls))\n",
    "    \n",
    "    def get_mean_x(self, y, j):\n",
    "        return 1 + self.ls / (self.c_x[y][j] + (len(self.x_classes) * self.ls))\n",
    "        \n",
    "    def count_x(self, X, y):\n",
    "        return [[len(X[:,j][y==k].reshape(-1,1) == self.x_classes[j])\n",
    "                       for j in range(len(self.x_classes))]\n",
    "                      for k in self.y_classes]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.apply_along_axis(lambda x: self.compute_probs(x), 1, X)\n",
    "    \n",
    "    def compute_probs(self, x):\n",
    "        probs = np.array([self.compute_prob(x, y) for y in range(len(self.y_classes))])\n",
    "        return self.y_classes[np.argmax(probs)]\n",
    "    \n",
    "    def compute_prob(self, x, y):\n",
    "        Pxy = 1\n",
    "        for j in range(len(x)):\n",
    "            x_clas = self.x_classes[j]\n",
    "            if x[j] in x_clas:\n",
    "                i = list(x_clas).index(x[j])\n",
    "                p_x_j_y = self.phi_x[y][j][i] # p(xj|y)\n",
    "                Pxy *= p_x_j_y\n",
    "            else:\n",
    "                Pxy *= self.get_mean_x(y, j)\n",
    "        return Pxy * self.phi_y[y]\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return (self.predict(X) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accurracy fold {i}:', set([0.6075]))\n",
      "('accurracy fold {i}:', set([0.6375]))\n",
      "('accurracy fold {i}:', set([0.6125]))\n",
      "('accurracy fold {i}:', set([0.6225]))\n",
      "('accurracy fold {i}:', set([0.645]))\n",
      "('Average accuracy is:', set([0.645]))\n"
     ]
    }
   ],
   "source": [
    "X_cross=hot_tra\n",
    "y_cross=y\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=5)\n",
    "accuracy = []\n",
    "for i, (train_index, validate_index) in enumerate(kfold.split(X_cross)):\n",
    "    X_train, y_train = X_cross[train_index], y_cross[train_index]\n",
    "    X_valid, y_valid = X_cross[validate_index], y_cross[validate_index]   \n",
    "    \n",
    "    model_curr = MultinomialNB().fit(X_train,y_train)\n",
    "\n",
    "    y_hat = model_curr.predict(X_valid)\n",
    "    accuracy = model_curr.evaluate(X_valid,y_valid)\n",
    "\n",
    "    print ('accurracy fold {i}:', {accuracy})\n",
    "print ('Average accuracy is:', {np.mean(accuracy)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_curr.predict(hot_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Bound\n",
       "0      0      1\n",
       "1      1      1\n",
       "2      2      0\n",
       "3      3      0\n",
       "4      4      1\n",
       "5      5      1\n",
       "6      6      1\n",
       "7      7      1\n",
       "8      8      0\n",
       "9      9      1\n",
       "10    10      1\n",
       "11    11      0\n",
       "12    12      0\n",
       "13    13      1\n",
       "14    14      1\n",
       "15    15      1\n",
       "16    16      1\n",
       "17    17      1\n",
       "18    18      0\n",
       "19    19      0\n",
       "20    20      1\n",
       "21    21      0\n",
       "22    22      0\n",
       "23    23      0\n",
       "24    24      0\n",
       "25    25      0\n",
       "26    26      0\n",
       "27    27      0\n",
       "28    28      0\n",
       "29    29      0\n",
       "..   ...    ...\n",
       "970  970      0\n",
       "971  971      1\n",
       "972  972      1\n",
       "973  973      1\n",
       "974  974      0\n",
       "975  975      1\n",
       "976  976      0\n",
       "977  977      0\n",
       "978  978      0\n",
       "979  979      1\n",
       "980  980      0\n",
       "981  981      0\n",
       "982  982      1\n",
       "983  983      0\n",
       "984  984      0\n",
       "985  985      1\n",
       "986  986      0\n",
       "987  987      1\n",
       "988  988      1\n",
       "989  989      0\n",
       "990  990      0\n",
       "991  991      0\n",
       "992  992      1\n",
       "993  993      1\n",
       "994  994      1\n",
       "995  995      0\n",
       "996  996      0\n",
       "997  997      0\n",
       "998  998      1\n",
       "999  999      1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes = pd.read_csv('Xte.csv')\n",
    "tes['Bound'] = y_hat\n",
    "subm19 = tes[['Id','Bound']]\n",
    "subm19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm19.to_csv('Submission19.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "X_train, X_test, y_train, y_test = tts(hot_tra, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "     \n",
    "class GaussianNB:\n",
    "    \n",
    "    def fit(self, X, y, epsilon = 1e-10):\n",
    "        self.y_classes, y_counts = np.unique(y, return_counts=True)\n",
    "        self.x_classes = np.array([np.unique(x) for x in X.T])\n",
    "        self.phi_y = 1.0 * y_counts/y_counts.sum()\n",
    "        self.u = np.array([X[y==k].mean(axis=0) for k in self.y_classes])\n",
    "        self.var_x = np.array([X[y==k].var(axis=0)  + epsilon for k in self.y_classes])\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.apply_along_axis(lambda x: self.compute_probs(x), 1, X)\n",
    "    \n",
    "    def compute_probs(self, x):\n",
    "        probs = np.array([self.compute_prob(x, y) for y in range(len(self.y_classes))])\n",
    "        return self.y_classes[np.argmax(probs)]\n",
    "    \n",
    "    def compute_prob(self, x, y):\n",
    "        c = 1.0 /np.sqrt(2.0 * np.pi * (self.var_x[y]))\n",
    "        return np.prod(c * np.exp(-1.0 * np.square(x - self.u[y]) / (2.0 * self.var_x[y])))\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        return (self.predict(X) == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6175"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = GaussianNB()\n",
    "m1.fit(X_train,y_train)\n",
    "p=m1.predict(X_test)\n",
    "m1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
